---
layout: post
title: Paper submitted to the EAERE Conference in Manchester
header-image: /assets/img/news/2018-07-30-gothenburg-01.jpg
author: Erlend Dancke Sandorf
image: /assets/img/news/icmc.jpg
category: news
excerpt: We have submitted a paper to the European Association of Environmental and Resource Economists' Conference in Manchester.
---

Danny and I attended the International Choice Modeling Conference in Kobe in Japan and gave several presentations. Two presentations made use of the novel data created as part of the INSPiRE project, wheras the third presentation was part of a special session on the use of latent variable models. The following is also published in identical form on the [ACRG website](https://acrg.site) and in similar form on [Erlend's personal website](https://edsandorf.me). 

# An information search approach to discrete choice analysis

As choice modelers we observe decision makers’ choices among competing alternatives, and we try to come up with a model that best describes the observed choice behavior. The economic model of the rational utility maximizer is still pervasive. This model of behavior is based on the assumption that a decision maker has complete information about all products in a market and full knowledge of their preferences. The chosen alternative is then the one that gives the highest utility. In the real world, it is rare that people have complete information about all available alternatives, i.e. people do not have a fully formed set of alternatives (choice set) from which they can choose. Instead, people need to search for information about new alternatives. This implies that a decision maker’s consideration set is growing with one alternative per period of search. Optimal search theory stipulates that this search process will continue as long as the expected gain from continuing to search exceeds the cost of doing so. At the point where search stops, a choice is made among the alternatives in the consideration set. Note that this consideration set is a proper subset of the complete choice set. However, current choice models, using both revealed and stated preference data, all but ignore the choice set generation process, which is characterized by the sequential and ordered search process. That said, there are several papers out there assuming some form of choice set generation process, however, we argue that these approaches are missing the mark by ignoring the search process. The search process is driven by the expectation that there might be a better alternative out there, i.e. a decision maker will continue to search as long as the expected utility of the unseen alternatives is higher than the current highest utility plus search costs. Not only will failing to consider the search process ignore an important source of heterogeneity, which arises because different decision makers considered different alternatives in different orders, but it runs the risk of underestimating the probabilities of choice. Interestingly, a decision made among alternatives in a consideration set generated in this manner, is a decision made with incomlete information. If the choice among alternatives is made in a utility maximizing manner, then the choice may only be a local, as opposed to a global, maximum.

To illustrate how our model works, assume that you are in the market for shoes. It is unlikely that you have complete information about all pairs of shoes out there and full knowledge of your preferences for each of those pairs. Instead, you start searching. You go to a store and you look at a pair of shoes (your first alternative). Assuming that you have no other information about shoes, you do not know whether this is a ``good’’ pair of shoes. The question is: what is the probability that another pair of shoes that you have not yet seen gives higher utility than the pair you have seen plus the cost of searching for another alternative? If the answer is sufficiently large, you look at another pair of shoes, i.e. you grow your consideration set by revealing another alternative. As you establish more information on shoes you are better able to determine the probability that there is a better pair of shoes out there. In other words, as you learn about new alternatives you learn about your preferences and you update your search probability. When the search probability becomes suffiently small you stop search and make a choice among the shoes in your consideration set. Note that this process implies that your consideration set might not contain the global utility maximizing choice and that you might continue searching after you have encountered your utility maximizing choice if your expectaions about what other options exist are unrealistically high. Importantly, it allows the analyst to determine a smaller possibly more correct consideration set.

In this paper we develop a novel model that explicitly considers the information search process and the role it plays in choice set generation. For convenience, we let the choice among alternatives in the consideration set be described by a utility maximization rule. We test our model on equally novel data collected through a sequential search process, where at each stage, the decision maker has to choose whether to reveal another alternative or make a choice among the ones she has already seen. We show through Monte-Carlo simulations that our model is able to retrieve consistent parameter estimates and correctly predict decision makers’ choices conditional on their consideration set. We run simulations under various assumptions with respect to the parameters, number of alternatives and sample sizes. We then compare our model’s ability to retrieve consistent estimates against more standard discrete choice models. Finally, we show how our model performs on real data gathered in a conventional way and gathered using a sequential approach. The sequential approach more closely mirrors real life behavior and our model is developed specifically to address this.

Erlend's presentation: [An information search approach to discrete choice analysis](https://www.acrg.site/talk/icmc-2019-iii/icmc-2019-iii.pdf)

# A simple satisficing model
Economic theory is built on the assumption that people are omniscient utility maximizers. That they have complete information about all available options, knowledge of their preferences and the ability to calculate their expected utility from choosing any one option. While these assumptions are necessary for welfare economics, they may not fully describe how people make choices in real life. Indeed, people routinely make decisions that cannot readily be described by the standard model of rationality. People often lack the memory and cognitive abilities to be perfectly rational, but instead act boundedly rational. The idea of bounded rationality is built on the premise that people rarely have complete information about alternatives nor do they have perfect knowledge of their preferences, but learn about both through (costly) search for information and deliberation.

In some situations people may make a sequence of decisions based on whether or not the utility of the current alternative exceeds some threshold utility. As such, the decision process is one in which each alternative is evaluated sequentially and the first one exceeding the threshold utility is chosen. This decision rule is called satisficing, i.e., choosing the first alternative that is satisfactory. A choice following this type of decision rule may or may not be utility maximizing. If the first satisfactory alternative encountered happens to be the one that gives the highest global utility, then that choice is also utility maximizing. However, any other choice is by definition satisfactory, but not maximizing.

This decision rule has received limited interest from choice modellers. Only a few papers have developed models that can identify satisficing behavior in more traditional discrete choice data, and none are necessarily readily implemented within existing software. In this paper, we develop a simple satisficing model that can be run on standard data that involves choosing the first alternative with utility exceeding some threshold level of utility. This threshold utility is estimated. To test the performance of our model, we run a series of Monte-Carlo simulations. We see how well our model retrieves the true parameters under various assumptions about the level of the threshold utility. Our results show that if the threshold utility is sufficiently low, then the first alternative seen will be chosen. At the other extreme, where the threshold is sufficiently high, then no decision can be made following a satisficing rule, i.e., no alternatives give higher utility than the threshold. This implies that the observed choice must be made following some other decision rule. We discuss implications of our results and ways forward.

Danny's presentation: [A simple satisficing model](https://www.acrg.site/talk/icmc-2019-i/icmc-2019-i.pdf)

# The use of latent variable models in policy: a road fraught with peril?

Choice modelers have long recognized that people’s attitudes and beliefs affect their decisions. However, self reported measures are subject to measurement error, and these self-reported measures are likely correlated with unobserved factors influencing choice leading to potential endogeneity bias. The integrated choice and latent variable model allows for the inclusion of these measures indirectly through a latent variable. Variations of the model has been implemented in marketing, transport, environmental and health economics.

The basic premise of the model is that answers to Likert scale questions concerning attitudes and beliefs can be mapped to latent character traits. For example, responses to questions about nature conservation might reveal information that you are latently a tree hugger. However, the latent variable is difficult to interpret and it is unclear whether it measures what the researcher thinks. Some authors argue that integrated choice and latent variable models should be accompanied by exploratory factor analysis to understand better which attitudinal questions correlate with which latent constructs. Nonetheless, this lack of interpretability raises questions about the appropriateness of using hybrid choice models to inform policy.

An inappropriate use of hybrid choice models is to inform policies that seek to influence choice by targeting the latent variable given the possible endogenous relationship between the two. While this appears to be common in transport, researchers in other fields, e.g., environmental economics, seek to understand how latent character traits affect choices and behavior. As such, hybrid choice models could serve a purpose in identifying groups of people that are more susceptiple to a certain policy meausure.

While a familiar aphorism among econometricians is that all models are wrong, to be of practical use, there is a need to ensure that choice model results are understandable to a non-technical entity. Choice modellers, like other econometricans, should adhere the words of Albert Einstein that Everything should be made as simple as possible, but not simpler. There is the need to be mindful of the proliferation of parameters and model complexity. While more comprehensive models ensure the choice data is ftted well, there is a risk that it is tailored too closely to the sample data. This compromises the ability to generalise the model beyond the existing dataset and may be restrictive for policy makers. While end users will often want to establish the relationship between the dependent variable(s) and a relatively small number of key independent variables, increasing model complexity is justified if it produces reasonably more accurate results. The investigations in this paper follows from this latter argument, and the need to quantify reasonably. Are there certain conditions under which identifying these latent segments of the population outweighs the computational cost of doing so? We put forth arguments for whether or when it is appropriate to consider hybrid discrete choice models to inform policy, but continue the discussion of whether the hybrid model exists out of academic curiosity or the real need to use unobservable latent variables to inform policy.

Special session on latent variables: [The use of latent variable models in policy: a road fraught with peril?](https://www.acrg.site/talk/icmc-2019-v/icmc-2019-v.pdf)